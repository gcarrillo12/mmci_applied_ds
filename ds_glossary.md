## List of Data Science Terms
*MMCi Applied Data Science*

---

- **Active learning:** A special case of machine learning in which a learning algorithm can interactively query a user to label new data points with the desired outputs
- **Algorithm:** A process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer
- **Area under the ROC Curve (AUROC):** A common performance metric for binary classification that can be computed by (a) quantifying the area under the receiver operating characteristic (ROC) curve *or* (equivalently) the sensitivity-specificity curve; or (b) calculating the probability that a randomly selected positive example has a higher predicted probability of being predicted positive than a randomly selected negative example
- **Average Precision (AP):** Also called the average positive predictive value (PPV), the average precision is an estimate of the area under the precision (i.e. PPV) versus recall (i.e. sensitivity) curve
- Bag of words model: The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.
- Binary Classification: It is a process or task of classification, in which a given data is being classified into two classes.  It’s basically a kind of prediction about which of two groups the thing belongs to.
- Black Box Model: After a model is trained it can sometimes become difficult to understand the inner workings of the model and the manner in which it arrives at decisions, especially in deep neural networks. The model is then considered a ‘Black Box’
- **Categorical variables:** A variable that can take on one of a limited and usually fixed number of possible values. Common examples in healthcare include sex, race, diagnosis codes, procedure codes, and medications
- Classification Model: Classification models are a subset of supervised machine learning . A classification model reads some input and generates an output that classifies the input into some category
- Clustering: A way of grouping the data points into different clusters, consisting of similar data points
- Computable phenotype: re-usable computerized search queries that detect specific clinical events or diseases using electronic health record data
- Computer Vision: (broader term in relation to CNN)
- Confusion Matrix: A summary of prediction results on a classification problem.
- Convolution: In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function ( ) that expresses how the shape of one is modified by the other
- Convolutional Neural Network (CNN): A  Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.
- Cross entropy Loss: Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.
- **Data filtering:** Applying inclusion and/or exclusion criteria to remove individuals or data points from an analysis
- Data Model: the process of producing a descriptive diagram of relationships between various types of information that are to be stored in a database. One of the goals of data modeling is to create the most efficient method of storing information while still providing for complete access and reporting. 1
- Data parameter: A set of properties whose values determine the characteristics of something. A limit or boundary of the data.
- Data Partitioning: the technique of distributing data across multiple tables, disks, or sites in order to improve query processing performance or increase database manageability
- Decision surface: A (hyper) surface in a multidimensional state space that partitions the space into different regions. Data lying on one side of a decision surface are defined as belonging to a different class from those lying on the other
- Deep Learning: A machine learning technique that teaches computers to do what comes naturally to humans: learn by example.
- Dependent variables: A value that depends on changes in the independent variable ( see independent variable below).
- Early stopping: In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration
- Embedding: Dense numerical representations of real-world objects and relationships, expressed as a vector.
- Explainable Model: refers to the concept of being able to understand the machine learning model.
- False Negative: A false negative is a test result which wrongly indicates that a condition does not hold
- False Positive: A result that indicates a given condition exists when it does not.
- Feature: Features are the input variables to a machine learning model. For example, when developing a model predicting stroke risk, a feature would be a patient’s height or weight. Features can be processed before they are entered into a model, such as combining height and weight into a body mass index. For an image, a feature may be some component of the image, such as an eye or a nose, when developing a facial recognition machine learning system.
- Filter or Convolutional Filter: Filters detect spatial patterns such as edges in an image by detecting the changes in intensity values of the image.
- Fully connected layers: Fully Connected layers in a neural networks are those layers where all the inputs from one layer are connected to every activation unit of the next layer.
- Generalized Linear Model: A normal linear regression models with a continuous response variable.
- Gradient descent: Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).
- Ground truth: information that is known to be real or true, provided by direct observation and measurement (i.e. empirical evidence)
- Hyperparameter: Hyperparameters are parameters that are established before a model is trained and remain fixed through the training process. The hyperparameters generally affect the parameters that are learned during training and can have a large influence on the final accuracy. One of the difficulties in machine learning is in determining sets of hyperparameters that optimize the model fit.
- Image Segmentation: Image segmentation divides an image into different partitions known as segments.
- Imputation: The process of replacing missing data with substituted values.
- Independent Variable: A value that is independent of other variables in your study.
- Interpretable Model: Models who explain themselves, for instance from a decision tree you can easily extract decision rules.
- Label: The label identifies what a collection of data (the model input) represents. For a stroke model, it would be stroke present or absent. When developing a machine learning system to identify diabetic retinopathy, the label for each fundus image would be present or absent, as determined by experts in interpreting such images.
- Latent Features ( put with features): Features that we don't directly observe but can be extracted typically by some algorithm
- **Learning (in machine learning)**: Adjusting the parameters of a predictive model in order to improve model performance
- Life-long learning: accumulating past knowledge that it then uses in future learning and problem solving
- Link Function: A link function in a Generalized Linear Model maps a non-linear relationship to a linear one.
- Logistic Regression: Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable
- Loss:
- Likelihood:
- Machine Learning, Artificial Intelligence, Deep Learning: Artificial intelligence is a loosely defined concept describing automated systems that can perform tasks considered to require “intelligence.” Machine learning refers to the process of developing systems with the ability to learn from and make predictions using data. For example, a machine learning model can process an input (such as a retinal fundus photograph) and produce an output (such as the classification of the image showing that proliferative diabetic retinopathy is present). Deep learning is a more specific group of machine learning methods that uses many layers of arithmetic operations.25,40
- **Model architecture:** The form of the equation that is applied to the input features to generate an output. Equivalently, the structure of the graph that links the input features to the output. *Example*: A multilayer perceptron with a single hidden layer of size 100 units and sigmoid activations
- Model calibration: The act of improving our model such that the distribution and behavior of the probability predicted is similar to the distribution and behavior of probability observed in training data.
- Model discrimination: The procedure in which the most appropriate model has to be identified from a set of rival models, implying that additional experiments will have to be designed and performed during the model discrimination procedure.
- Model Generalization: refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.
- Model interpretability: The degree to which a human can consistently predict the model’s result
- Model, Algorithm: In the machine learning setting, model and algorithm are frequently used interchangeably to refer to the final ready-to-use machine learning method. These terms refer to the steps taken by the machine to assess input data and make a determination about what is shown in the data.
- **Motif:** A pattern present in an image that can be detected with a convolutional filter.
- Multi-class classification: The problem of classifying instances into one of three or more classes/
- Multi-Layer Perceptron (MLP): A supplement of feed forward neural network. It consists of three types of layers—the input layer, output layer and hidden layer. The input layer receives the input signal to be processed. The required task such as prediction and classification is performed by the output layer. An arbitrary number of hidden layers that are placed in between the input and output layer are the true computational engine of the MLP. Similar to a feed forward network in a MLP the data flows in the forward direction from input to output layer. The neurons in the MLP are trained with the back propagation learning algorithm.
- Natural Language Processing (NLP):  A field in machine learning with the ability of a computer to understand, analyze, manipulate, and potentially generate human language.
- Neural Network: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.
- Numeric variables: Numeric variables have values that describe a measurable quantity as a number, like 'how many' or 'how much'. Therefore numeric variables are quantitative variables.
- Object Detection: A computer vision technique that allows us to identify and locate objects in an image or video
- **Odds:** The ratio of the probability that an event of interest *will* occur to the probability that it *will not* occur
- Online Learning: A method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once
- Operating point:
- Ordinal variables: An ordinal variable is similar to a categorical variable. The difference between the two is that there is a clear ordering of the categories.
- Outlier: An observation that lies an abnormal distance from other values in a random sample from a population.
- Overfitting: Overfitting is a scenario in which a machine learning model is trained to predict the training data too well, such that it does not generalize to new data sets. In theory, any set of data can be fit with a mathematical model if large numbers of parameters are entered into a mathematical model. This overfitting can occur even if there is no logical relationship between the data and the outcome. For example, a reasonably good fit can be obtained using regression to determine the relationship between age, cholesterol, and sex, to stroke because each of these variables has a physiological relationship with the development of atherosclerosis and subsequent stoke. The mathematical model relating these risk factors and stroke can have a better fit if more parameters than these are entered into the model, even if those parameters have nothing to do with stroke. The resulting model may not perform well clinically if its fit relies on these extra variables. When the model is applied to a different data set than the one on which it was developed, its predictive ability may fail.
- Parameter: Parameters are the internal values of a machine learning model that are derived based on the training data. For example, the parameters in logistic regression include the weights that are multiplied with each input variable as part of the regression equation. If a logistic-regression model were developed to assess the need for a radiograph to evaluate an ankle trauma case, input features may include the presence of bone tenderness at anatomic sites A and B. The parameter associated with each site would be greater than 0, indicating a higher likelihood that radiographs are needed to rule out fracture. The overall score would be related to multiplying the presence or absence (1 or 0) of tenderness at A and B with their respective parameters. The values of the parameters are learned during a training process to optimize the fit between the available data and the machine learning model outputs.
- Parameters: A parameter is a number describing a whole population (e.g., population mean)
- Positive and negative predictive value: In medicine, positive predictive value is the probability that subjects with a positive screening test truly have the disease or condition. Negative predictive value is the probability that subjects with a negative screening test truly don't have the disease or condition.
- Precision another term for positive predictive value:  How close or dispersed the measurements are to each other.
- Predictive Model: A mathematical process used to predict future events or outcomes by analyzing patterns in a given set of input data.
- Outcome variable: outcome variable is an event or metric that can be observed and measured in a valid fashion. 
- Predictors: Explanatory or independent variables. 
- Preprocessing: Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models.https://www.upgrad.com/blog/data-preprocessing-in-machine-learning/
- Pre-training: Pre-training is a method which trains shallow neural networks using an unsupervised objective before stacking them to create deep neural networks
- Probability: A mathematical tool used to study randomness. It deals with the chance (the likelihood) of an event occurring.
- Probability distribution: The “shape” of your data set.  A statistical function that describes the likelihood of the occurrence of possible values that a variable can take and plots them to a visual graph. A common example visually is the bell curve
- **Recall:** Another term for sensitivity
- Receiver Operating Characteristic Curve (ROC curve): ROC curve is a graph showing the performance of a classification model at all classification thresholds
- Reference Standard: For a diagnostic test, a reference standard is the reference against which the proposed method is compared. The reference standard is often a widely accepted test or gold standard for the diagnosis, but it can also be based on diagnoses provided by expert clinicians.
- Regression: Regression Models are used to predict a continuous value. An example would predicting prices of a house given the features of house like size, price etc is one of the common examples of Regression. It is a supervised technique. 
- Regularization: A technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting.
- Reinforcement Learning: A machine learning training method based on rewarding desired behaviors and/or punishing undesired ones. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.
- Semi-supervised learning: A type of machine learning that refers to a learning problem (and algorithms designed for the learning problem) that involves a small portion of labeled examples and a large number of unlabeled examples from which a model must learn and make predictions on new examples.
- **Sensitivity:** The proportion of positive cases correctly identified (i.e. predicted) as positive by a prediction model or diagnostic test
- Sequential Data: Sequential Data is any kind of data where the order matters to the observation.
- Sparse: Features with sparse data are features that have mostly zero values. This is different from features with missing data. Examples of sparse features include vectors of one-hot-encoded words or counts of categorical data
- Specificity: The proportion of actual negatives, which got predicted as the negative (or true negative)
- Structured and unstructured data: Structured data is highly specific and is stored in a predefined format, where unstructured data is a conglomeration of many varied types of data that are stored in their native formats.
- Supervised Learning: Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.
- **Tabular data:** Data stored in a table or spreadsheet format
- Test Set: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.
- Time-Series Data: Time series data is a collection of observations obtained through repeated measurements over time.
- Training (training similar to learning): Model training in machine language is the process of feeding an ML algorithm with data to help identify and learn good values for all attributes involved.
- Training Set: 
- Transfer learning: Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem
- True Negative: An outcome where the model correctly predicts the negative class.
- True Positive: an outcome where the model correctly predicts the positive class. 
- Tuning: The process of adjusting the hyperparameters of a trained model to increase the model’s fit to the tuning set. When tuning a machine learning model, hyperparameters are repeatedly adjusted, each time training a new machine learning model on the training set and evaluating that machine learning model on the tuning set. The optimal hyperparameter configuration is typically the configuration that leads to the best tuning set accuracy.
- Unsupervised Learning: A machine learning technique in which the users do not need to supervise the model.
- Validation Set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network.
- Variables:  any characteristics, number, or quantity that can be measured or counted
- Vector: A vector is a quantity or phenomenon that has two independent properties: magnitude and direction.
- Vectorization: process of flattening a matrix of numeric values to a vector
- Vocabulary (in nlp context): The set of unique words used in the text corpus is referred to as the vocabulary.
- Weights: Often termed parameters (look it up and add detail): Weight is the parameter within a neural network that transforms input data within the network's hidden layers.



